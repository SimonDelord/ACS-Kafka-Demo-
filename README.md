# ACS-Kafka-Demo-
This is the Git repo describing the various steps to interconnect ACS and a Kafka broker (AMQ Streams in that case).


## Demo Summary 
The idea behind this demo is to integrate ACS to AMQ Streams to provide event driven capabilities for:
- a higher order orchestrator (like a SOAR - could be Splunk SOAR, IBM QRadar, etc...)
- an automation engine (like AAP)
- any other endpoint that may want to do something based on a policy violation coming from ACS.
- a combination of some or all of the above

Today (e.g january 2024), ACS integrates via WebHooks and ideally moving forward, there will be a native ACS connector for Kafka.
In the meantime, here's something that can be used to provide this functionality.

This specific demo is around GitOps. It is presented in the figure below.

![Browser](https://github.com/SimonDelord/ACS-Kafka-Demo-/blob/main/images/ACS-Kafka-Demo.png)


GitOps relies on "users" (e.g could be developers, could be platform owners, could be SREs, etc.) to only ever make change in Git and let a "tool" (like ArgoCD and/or ACM) to monitor changes to the source Git Repo and act on those changes by reflecting those changes into the target environment(s).

This represented here by the number 1 (e.g a developer commits code to Git) and the number 2 (e.g ArgoCD monitors this Git repo and upon the change, reflects this change in the target OCP cluster). 

Now, if this change violates one of the ACS policies, (e.g number 3), ACS sends a notification (e.g number 4) to one or more endpoints via the broker. Each endpoint may then proceed to do whatever it is that they require to do with it (could be a playbook like in this demo, could be create a ticket via a security orchestrator (SOAR), could be an extra notification to other systems used by developers, platforms, etc..).

## Detailed ACS to Kafka integration view 
So in this context, we have developed a microservice (under the webhook-and-producer repo - representing the number 5) that:
- listens on a specific URL (E.g this is the connection defined in ACS as an Integration > GenericWebhook) 
- takes the ACS notification and publishes it on a specific topic (parameter defined under the .env file) as a JSON payload - (5a)

We have also developed a microservice (under the kafka-consumer repo - representing the number 5) that:
- listens to the topic (parameter defined under the .env file) that the previous microservice is a publisher for (5b)
- extracts the data relevant for the automation endpoint (e.g AAP) (5c)
- publishes the relevant data for the automation endpoint onto a different topic (parameter defined under the .env file) (5d)

The EDA part of AAP (number 6) is then able to retrieve the message generated by the previous microservice and drive a playbook for AAP that will update the source Git Repo with "the relevant" changes to make sure the application now complies with the ACS policy (in this example, set the ReplicaSet or Deployment to scale down to 0).

The details is presented in the next figure.

![Browser](https://github.com/SimonDelord/ACS-Kafka-Demo-/blob/main/images/ACS-Kafka-Interaction.png)

## EDA for AAP
My good friend and colleague Derek Waters managed to setup the EDA environment.
More information is available [here](https://github.com/derekwaters/ansible_eda_demo) for the EDA setup.

The actual AAP playbook used in this demo is avaible [here](https://github.com/SimonDelord/ansible/blob/main/acs-kafka-aap.yaml).
You will see that the playbook doesn't deal with Git but actually directly with the "faulty" deployment (e.g in breach of the ACS policy named "webhook"), but a similar playbook could be written to do so (I'm just too lazy sorry).
What's interesting though, is the validation of the EDA object passed to AAP:
- "{{ ansible_eda.event.body.namespace }}"
- "{{ ansible_eda.event.body.deployment }}"
